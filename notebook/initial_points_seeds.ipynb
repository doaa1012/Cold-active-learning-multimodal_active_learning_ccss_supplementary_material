{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd8ec2e7-3588-476a-bf29-b1ac25e9b95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from config import *\n",
    "import os\n",
    "import json\n",
    "from scripts.run_active_learning import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9484750c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing dataset: 10272\n",
      "Headers of DataFrame:\n",
      " ['ID' 'x' 'y' 'Ag' 'Au' 'Pd' 'Resistance']\n",
      "\n",
      " Processing dataset: 10275\n",
      "Headers of DataFrame:\n",
      " ['ID' 'x' 'y' 'Ag' 'Au' 'Pd' 'Pt' 'Rh' 'Resistance']\n",
      "\n",
      " Processing dataset: 10304\n",
      "Headers of DataFrame:\n",
      " ['ID' 'x' 'y' 'Au' 'Pd' 'Pt' 'Rh' 'Resistance']\n",
      "\n",
      " Processing dataset: 10311\n",
      "Headers of DataFrame:\n",
      " ['ID' 'x' 'y' 'Au' 'Pd' 'Pt' 'Rh' 'Ru' 'Resistance']\n",
      "\n",
      " Processing dataset: 10403\n",
      "Headers of DataFrame:\n",
      " ['ID' 'x' 'y' 'Ag' 'Au' 'Cu' 'Pd' 'Pt' 'Resistance']\n",
      "\n",
      " Processing dataset: 10402\n",
      "Headers of DataFrame:\n",
      " ['ID' 'x' 'y' 'Ag' 'Au' 'Pd' 'Pt' 'Resistance']\n",
      "\n",
      " Processing dataset: 10399\n",
      "Headers of DataFrame:\n",
      " ['ID' 'x' 'y' 'Au' 'Cu' 'Pd' 'Pt' 'Resistance']\n",
      "\n",
      " Processing dataset: 10374\n",
      "Headers of DataFrame:\n",
      " ['ID' 'x' 'y' 'Ru' 'Rh' 'Pd' 'Ir' 'Pt' 'Resistance']\n"
     ]
    }
   ],
   "source": [
    "TOP5_SUMMARY = Top5_Similarity_Summary\n",
    "MIN_MAX_SUMMARY = EDX_min_max_summary\n",
    "MAPPED_CENTROIDS_JSON = MAPPED_CENTROIDS_JSON\n",
    "OUTPUT_DIR = DATA_CLEAN_InIT_CHOICES\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Load global summaries\n",
    "top5_df = pd.read_csv(TOP5_SUMMARY)\n",
    "minmax_df = pd.read_csv(MIN_MAX_SUMMARY)\n",
    "with open(MAPPED_CENTROIDS_JSON, 'r') as f:\n",
    "    centroids_mapped = json.load(f)\n",
    "\n",
    "# --- Helpers ---\n",
    "\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, np.integer): return int(obj)\n",
    "    elif isinstance(obj, np.floating): return float(obj)\n",
    "    elif isinstance(obj, np.ndarray): return obj.tolist()\n",
    "    elif isinstance(obj, np.bool_): return bool(obj)\n",
    "    return obj\n",
    "\n",
    "def run_multiple_seeds(X, n_init=5, num_seeds=10, base_seed=42):\n",
    "    results_by_seed = {}\n",
    "    seeds = list(range(base_seed, base_seed + num_seeds))\n",
    "    for seed in seeds:\n",
    "        result = select_initial_indices(X, n_init=n_init, seed=seed)\n",
    "        seed_dict = {f\"{k}_seed_{seed}\": v for k, v in result.items()}\n",
    "        results_by_seed[seed] = seed_dict\n",
    "    return results_by_seed\n",
    "\n",
    "_seed_suffix_re = re.compile(r\"_seed_(\\d+|default)$\", re.IGNORECASE)\n",
    "\n",
    "def _dedup_keep_order(seq):\n",
    "    seen, out = set(), []\n",
    "    for x in seq:\n",
    "        if x not in seen:\n",
    "            seen.add(x)\n",
    "            out.append(x)\n",
    "    return out\n",
    "\n",
    "def _norm_centroid_key(variant: str, seed_key: str) -> str:\n",
    "    v = variant.strip().lower().replace(\" \", \"_\")\n",
    "    if v.startswith(\"centroids_\"):\n",
    "        v = v.split(\"centroids_\", 1)[1]\n",
    "    return f\"Centroids_{v}_{seed_key}\"\n",
    "\n",
    "def filter_base_for_seed(base_init_choices: dict, cur_seed: int) -> dict:\n",
    "    filtered = {}\n",
    "    for k, v in base_init_choices.items():\n",
    "        kl = k.lower()\n",
    "        if not kl.startswith(\"centroids_\"):\n",
    "            filtered[k] = v\n",
    "            continue\n",
    "        m = _seed_suffix_re.search(kl)\n",
    "        if not m:\n",
    "            continue\n",
    "        seed_str = m.group(1)\n",
    "        if seed_str == str(cur_seed):\n",
    "            filtered[k] = v\n",
    "    return filtered\n",
    "\n",
    "def build_base_init_choices(folder: str) -> dict:\n",
    "    base_init_choices = {}\n",
    "\n",
    "    # Top5 Similarity\n",
    "    top5_indices = top5_df[top5_df['Folder'].astype(str).str.startswith(folder)]['index'].tolist()\n",
    "    if top5_indices:\n",
    "        base_init_choices['Top5Similarity'] = top5_indices\n",
    "\n",
    "    # Max/Min Comp\n",
    "    minmax_subset = minmax_df[minmax_df['Folder'].astype(str).str.startswith(folder)]\n",
    "    max_comp = [int(row['MaxIndex']) for _, row in minmax_subset.iterrows()]\n",
    "    min_comp = [int(row['MinIndex']) for _, row in minmax_subset.iterrows()]\n",
    "    if max_comp:\n",
    "        base_init_choices['Max Comp'] = max_comp\n",
    "    if min_comp:\n",
    "        base_init_choices['Min Comp'] = min_comp\n",
    "\n",
    "    # Centroid Mappings\n",
    "    matching_wafer_id = f\"00{folder}\"\n",
    "    for wafer_key, wafer_dict in centroids_mapped.items():\n",
    "        if not str(wafer_key).startswith(matching_wafer_id):\n",
    "            continue\n",
    "        for variant, seeds_dict in wafer_dict.items():\n",
    "            if not isinstance(seeds_dict, dict):\n",
    "                entries = seeds_dict if isinstance(seeds_dict, list) else []\n",
    "                nearest = [int(e['nearest_stage_index']) for e in entries if isinstance(e, dict) and 'nearest_stage_index' in e]\n",
    "                key = _norm_centroid_key(variant, \"seed_default\")\n",
    "                base_init_choices[key] = _dedup_keep_order(nearest)[:5]\n",
    "                continue\n",
    "            for seed_key, entries in seeds_dict.items():\n",
    "                nearest = [int(e['nearest_stage_index']) for e in entries if isinstance(e, dict) and 'nearest_stage_index' in e]\n",
    "                key = _norm_centroid_key(variant, seed_key)\n",
    "                base_init_choices[key] = _dedup_keep_order(nearest)[:5]\n",
    "\n",
    "    return base_init_choices\n",
    "\n",
    "# --- Main processing loop ---\n",
    "datasets = [\n",
    "    DATASET_10272_Ag_Au_Pd_RT,\n",
    "    DATASET_10275_Ag_Au_Pd_Pt_Rh_RT,\n",
    "    DATASET_10304_Au_Pd_Pt_Rh_RT,\n",
    "    DATASET_10311_Au_Pd_Pt_Rh_Ru_RT,\n",
    "    DATASET_10403_Ag_Au_Cu_Pd_Pt_RT,\n",
    "    DATASET_10402_Ag_Au_Pd_Pt_RT,\n",
    "    DATASET_10399_Au_Cu_Pd_Pt_RT,\n",
    "    DATASET_10374_Ir_Pd_Pt_Rh_Ru \n",
    "]\n",
    "\n",
    "for dataset_path in datasets:\n",
    "    dataset_name = os.path.basename(dataset_path).split(\"_\")[0]\n",
    "    print(f\"\\n Processing dataset: {dataset_name}\")\n",
    "\n",
    "    data_exp = pd.read_csv(dataset_path)\n",
    "    if data_exp.empty:\n",
    "        print(f\" Warning: Dataset {dataset_name} is empty.\")\n",
    "        continue\n",
    "\n",
    "    # Load shared strategies from global summaries\n",
    "    base_init_choices = build_base_init_choices(folder=dataset_name)\n",
    "\n",
    "    # Rename any legacy centroid keys\n",
    "    renamed_init_choices = {}\n",
    "    for key, val in base_init_choices.items():\n",
    "        if \"Centroids_saturation_and_contrast_+++\" in key:\n",
    "            new_key = key.replace(\"Centroids_saturation_and_contrast_+++\", \"Centroids_saturation_high\")\n",
    "        elif \"Centroids_saturation_and_contrast_++\" in key:\n",
    "            new_key = key.replace(\"Centroids_saturation_and_contrast_++\", \"Centroids_saturation_medium\")\n",
    "        elif \"Centroids_saturation_and_contrast_+\" in key:\n",
    "            new_key = key.replace(\"Centroids_saturation_and_contrast_+\", \"Centroids_saturation_low\")\n",
    "        else:\n",
    "            new_key = key\n",
    "        renamed_init_choices[new_key] = val\n",
    "    base_init_choices = renamed_init_choices\n",
    "\n",
    "    # Log-transform target if present\n",
    "    target = [\"Resistance\"]\n",
    "    if any(t in data_exp.columns for t in target):\n",
    "        data_exp[target] = np.log(data_exp[target])\n",
    "\n",
    "    # Create dataset-specific output folder (e.g. DATA_CLEAN_InIT_CHOICES/10272)\n",
    "    dataset_output_dir = os.path.join(DATA_CLEAN_InIT_CHOICES, dataset_name)\n",
    "    os.makedirs(dataset_output_dir, exist_ok=True)\n",
    "\n",
    "    # Get feature columns\n",
    "    all_columns = data_exp.columns.tolist()\n",
    "    features = [col for col in all_columns if col not in [\"ID\", \"x\", \"y\", \"Resistance\"]]\n",
    "\n",
    "    # Instantiate device and extract features\n",
    "    device = Resistance(data_exp, features=features, target=target)\n",
    "    X_all = device.get_features()\n",
    "\n",
    "    # Run seeds and save combined strategy per seed\n",
    "    seed_results = run_multiple_seeds(X_all, n_init=5, num_seeds=10, base_seed=42)\n",
    "    for seed, seed_strategies in seed_results.items():\n",
    "        base_for_seed = filter_base_for_seed(base_init_choices, cur_seed=seed)\n",
    "        combined_results = base_for_seed.copy()\n",
    "        combined_results.update(seed_strategies)\n",
    "\n",
    "        # Save inside dataset-specific folder\n",
    "        seed_json_path = os.path.join(dataset_output_dir, f\"{dataset_name}_seed_{seed}.json\")\n",
    "        with open(seed_json_path, \"w\") as f:\n",
    "            json.dump(combined_results, f, indent=4, default=convert_to_serializable)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
